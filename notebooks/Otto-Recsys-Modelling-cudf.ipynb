{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 12 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import cudf, itertools\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    data_path = '../data/'\n",
    "    local_validation = False\n",
    "    validation_path = '../data/local_validation/'\n",
    "    train_file = 'train.parquet'\n",
    "    test_file = 'test.parquet'\n",
    "    test_labels_file = 'test_labels.parquet'\n",
    "    n_session_samples = 100\n",
    "    n_most_common = 50\n",
    "    debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.local_validation:\n",
    "    train = cudf.read_parquet(config.validation_path + config.train_file)\n",
    "    test = pd.read_parquet(config.validation_path + config.test_file)\n",
    "    test_labels = cudf.read_parquet(config.validation_path + config.test_labels_file)\n",
    "    data = cudf.concat([train])\n",
    "else:\n",
    "    train = cudf.read_parquet(config.data_path + config.train_file)\n",
    "    test = cudf.read_parquet(config.data_path + config.test_file)\n",
    "    data = cudf.concat([train, test])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    data = data.sample(frac=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "type_weight = {0:1, 1:6, 2:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create co-visitation matrix on GPU using CuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carts Orders Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14571582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [01:09<00:00,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "# print len of sessions\n",
    "print(len(sessions))\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'],ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    # take only the first 30 rows (tail) of each session\n",
    "    df = df.loc[df.n<30].drop('n', axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = df.type_y.map(type_weight)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "# we only select 15 products for each aid_x\n",
    "tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.parallel_apply(list)\n",
    "with open(config.data_path + f'top_15_carts_orders_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buy2Buy Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:52<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 4.08 s, total: 1min 11s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data_copy = data.copy()\n",
    "# data_copy = data_copy.set_index('session')\n",
    "# sessions = data_copy.index.unique()\n",
    "\n",
    "# chunk_size = 100_000\n",
    "\n",
    "# tmp = list()\n",
    "# for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "#     df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "#     df = df.loc[df.type.isin([1, 2])]\n",
    "\n",
    "#     df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "#     # USE TAIL OF SESSION\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df['n'] = df.groupby('session').cumcount()\n",
    "#     df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "#     # CREATE PAIRS\n",
    "#     df = df.merge(df, on='session')\n",
    "#     df = df.loc[ ((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "#     # ASSIGN WEIGHTS\n",
    "#     df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "#     df['wgt'] = 1\n",
    "#     df = df[['aid_x','aid_y','wgt']]\n",
    "#     df.wgt = df.wgt.astype('float32')\n",
    "#     df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "#     tmp.append(df.reset_index())\n",
    "    \n",
    "#     del df\n",
    "#     gc.collect()\n",
    "\n",
    "# tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "# tmp = pd.concat(tmp)\n",
    "# # CONVERT MATRIX TO DICTIONARY\n",
    "# tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# # SAVE TOP 15\n",
    "# tmp = tmp.reset_index(drop=True)\n",
    "# tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "# tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "# # SAVE TO DISK\n",
    "# df = tmp.groupby('aid_x').aid_y.parallel_apply(list)\n",
    "with open(config.data_path + f'top_15_buy2buy_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicks Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [01:07<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 19.8 s, total: 4min 27s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# data_copy = data.copy()\n",
    "# data_copy = data_copy.set_index('session')\n",
    "# sessions = data_copy.index.unique()\n",
    "\n",
    "# chunk_size = 100_000\n",
    "\n",
    "# tmp = list()\n",
    "# for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "#     df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "#     df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "#     # USE TAIL OF SESSION\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     df['n'] = df.groupby('session').cumcount()\n",
    "#     df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "#     # CREATE PAIRS\n",
    "#     df = df.merge(df,on='session')\n",
    "#     df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "#     # ASSIGN WEIGHTS\n",
    "#     df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "#     df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "#     df = df[['aid_x','aid_y','wgt']]\n",
    "#     df.wgt = df.wgt.astype('float32')\n",
    "#     df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "#     tmp.append(df.reset_index())\n",
    "\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "\n",
    "# tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "# tmp = pd.concat(tmp)\n",
    "# # CONVERT MATRIX TO DICTIONARY\n",
    "# tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# # SAVE TOP 40\n",
    "# tmp = tmp.reset_index(drop=True)\n",
    "# tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "# tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "# # SAVE TO DISK\n",
    "# df = tmp.groupby('aid_x').aid_y.parallel_apply(list)\n",
    "with open(config.data_path + f'top_20_clicks_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks: 1837166\n",
      "carts: 1837166\n",
      "buy2buy: 1168768\n",
      "CPU times: user 13.3 s, sys: 1.54 s, total: 14.8 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "top_20_clicks = pd.read_pickle(config.data_path + f'top_20_clicks_v{version}.pkl')\n",
    "top_15_buys = pd.read_pickle(config.data_path + f'top_15_carts_orders_v{version}.pkl')\n",
    "top_15_buy2buy = pd.read_pickle(config.data_path + f'top_15_buy2buy_v{version}.pkl')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test.loc[test['type']==type_labels['clicks'],'aid'].value_counts().index.values[:20]\n",
    "top_orders = test.loc[test['type']==type_labels['orders'],'aid'].value_counts().index.values[:20]\n",
    "\n",
    "# print shape of each matrix\n",
    "print(f'clicks: {len(top_20_clicks)}')\n",
    "print(f'carts: {len(top_15_buys)}')\n",
    "print(f'buy2buy: {len(top_15_buy2buy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df, top_20_clicks, top_clicks):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.1, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        \n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "    else:\n",
    "        products_1 = list(itertools.chain(*[top_20_clicks[product] \\\n",
    "                        for product in unique_products if product in top_20_clicks]))\n",
    "        top_products_1 = [product for product, _ in Counter(products_1).most_common(50) \\\n",
    "                        if product not in unique_products]\n",
    "        result = unique_products + top_products_1[:20 - len(unique_products)]\n",
    "        return result + list(top_clicks[:20 - len(result)])\n",
    "\n",
    "def suggest_buys(df, top_15_buy2buy, top_15_buys, top_orders):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # filter df for type 1 and 2\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        products_1 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                        for product in unique_buys if product in top_15_buy2buy]))\n",
    "        for product in products_1: products_tmp[product] += 0.1\n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "    else:\n",
    "        products_1 = list(itertools.chain(*[top_15_buys.get(product, []) \\\n",
    "                          for product in unique_products if product in top_15_buys]))\n",
    "        products_2 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                          for product in unique_buys if product in top_15_buy2buy]))\n",
    "        top_products = [product for product, _ in Counter(products_1 + products_2).most_common(50) \\\n",
    "                        if product not in unique_products]\n",
    "        result = unique_products + top_products[:20 - len(unique_products)]\n",
    "        return result + list(top_orders[:20 - len(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 10s, sys: 1.42 s, total: 12min 12s\n",
      "Wall time: 12min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not config.local_validation:\n",
    "    test = test.to_pandas()\n",
    "\n",
    "pred_df_clicks = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_clicks(x, top_20_clicks, top_clicks)\n",
    ")\n",
    "\n",
    "pred_df_buys = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_buys(x, top_15_buy2buy, top_15_buys, top_orders)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 1660529 94230 742709 6205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 487136 1502122 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 1681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>834354 595994 740494 889671 987399 779477 1344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 1660529 94230 742709 6205...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 487136 1502122 17...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 1681...\n",
       "3  12899782_clicks  834354 595994 740494 889671 987399 779477 1344...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.parallel_apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(config.data_path + 'submission.csv', index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# COMPUTE METRIC\n",
    "\n",
    "if config.local_validation:\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.parallel_apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.parallel_apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet(config.validation_path + 'test_labels.parquet')\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.parallel_apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "        \n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clicks\n",
    "- 0.461\n",
    "- 0.474 (current parameters - clicks)\n",
    "### Carts\n",
    "- 0.376\n",
    "### Orders\n",
    "- 0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15824f91cc7b75e025d2367af0109417504b30ad993611e7f5b39069152fd433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
