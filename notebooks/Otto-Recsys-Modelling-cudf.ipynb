{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import cudf, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = cudf.read_parquet('../data/train.parquet')\n",
    "test = cudf.read_parquet('../data/test.parquet')\n",
    "\n",
    "data = cudf.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "type_weight = {0:1, 1:6, 2:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create co-visitation matrix on GPU using CuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14571582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [01:16<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "# print len of sessions\n",
    "print(len(sessions))\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'],ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n', axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = df.type_y.map(type_weight)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offload the final processing on CPU using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1837166,)\n"
     ]
    }
   ],
   "source": [
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "print(df.shape)\n",
    "with open(f'../data/top_15_carts_orders_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:57<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 3.72 s, total: 1min 14s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.loc[df.type.isin([1, 2])]\n",
    "\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df, on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = 1\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 15\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(f'../data/top_15_buy2buy_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [01:48<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(f'../data/top_20_clicks_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks: 374915\n",
      "carts: 362351\n",
      "buy2buy: 138144\n",
      "CPU times: user 1.11 s, sys: 88.6 ms, total: 1.2 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "\n",
    "VER = 1\n",
    "\n",
    "top_20_clicks = pd.read_pickle(f'../data/top_20_clicks_v{VER}.pkl')\n",
    "top_15_carts_orders = pd.read_pickle(f'../data/top_15_carts_orders_v{VER}.pkl')\n",
    "top_15_buy2buy = pd.read_pickle(f'../data/top_15_buy2buy_v{VER}.pkl')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test.loc[test['type']==0,'aid'].value_counts().index.values[:20]\n",
    "top_orders = test.loc[test['type']==2,'aid'].value_counts().index.values[:20]\n",
    "\n",
    "# print shape of each matrix\n",
    "print(f'clicks: {len(top_20_clicks)}')\n",
    "print(f'carts: {len(top_15_carts_orders)}')\n",
    "print(f'buy2buy: {len(top_15_buy2buy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[553507,\n",
       " 4541,\n",
       " 1442126,\n",
       " 190430,\n",
       " 1174094,\n",
       " 757471,\n",
       " 1496186,\n",
       " 504137,\n",
       " 28092,\n",
       " 1180838,\n",
       " 980751,\n",
       " 1271754,\n",
       " 1837463,\n",
       " 930325,\n",
       " 526193,\n",
       " 977455,\n",
       " 23704,\n",
       " 1142186,\n",
       " 458410,\n",
       " 675264]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_clicks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15824f91cc7b75e025d2367af0109417504b30ad993611e7f5b39069152fd433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
