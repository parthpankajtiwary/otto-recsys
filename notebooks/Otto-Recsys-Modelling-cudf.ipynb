{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import cudf, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    data_path = '../data/'\n",
    "    local_validation = True\n",
    "    validation_path = '../data/local_validation/'\n",
    "    train_file = 'train.parquet'\n",
    "    test_file = 'test.parquet'\n",
    "    test_labels_file = 'test_labels.parquet'\n",
    "    n_session_samples = 100\n",
    "    n_most_common = 50\n",
    "    debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.local_validation:\n",
    "    train = cudf.read_parquet(config.validation_path + config.train_file)\n",
    "    test = pd.read_parquet(config.validation_path + config.test_file)\n",
    "    test_labels = cudf.read_parquet(config.validation_path + config.test_labels_file)\n",
    "    data = cudf.concat([train])\n",
    "else:\n",
    "    train = cudf.read_parquet(config.data_path + config.train_file)\n",
    "    test = cudf.read_parquet(config.data_path + config.test_file)\n",
    "    data = cudf.concat([train, test])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    data = data.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "type_weight = {0:1, 1:6, 2:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create co-visitation matrix on GPU using CuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carts Orders Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5580740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:30<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "# print len of sessions\n",
    "print(len(sessions))\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'],ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    # take only the first 30 rows (tail) of each session\n",
    "    df = df.loc[df.n<100].drop('n', axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = df.type_y.map(type_weight)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "# we only select 15 products for each aid_x\n",
    "tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + 'top_15_carts_orders_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buy2Buy Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:36<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 2.88 s, total: 52.6 s\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.loc[df.type.isin([1, 2])]\n",
    "\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df, on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = 1\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 15\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + 'top_15_buy2buy_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicks Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [01:11<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 39s, sys: 1min 36s, total: 8min 16s\n",
      "Wall time: 14min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "version = 1\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<100].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "# CONVERT MATRIX TO DICTIONARY\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "# SAVE TOP 40\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<50].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + 'top_20_clicks_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks: 1815630\n",
      "carts: 1788152\n",
      "buy2buy: 1033926\n",
      "CPU times: user 12.5 s, sys: 1.09 s, total: 13.6 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "\n",
    "VER = 1\n",
    "\n",
    "top_20_clicks = pd.read_pickle(cofig.data_path + 'top_20_clicks_v{VER}.pkl')\n",
    "top_15_buys = pd.read_pickle(config.data_path + 'top_15_carts_orders_v{VER}.pkl')\n",
    "top_15_buy2buy = pd.read_pickle(config.data_path + 'top_15_buy2buy_v{VER}.pkl')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test.loc[test['type']==type_labels['clicks'],'aid'].value_counts().index.values[:20]\n",
    "top_orders = test.loc[test['type']==type_labels['orders'],'aid'].value_counts().index.values[:20]\n",
    "\n",
    "# print shape of each matrix\n",
    "print(f'clicks: {len(top_20_clicks)}')\n",
    "print(f'carts: {len(top_15_buys)}')\n",
    "print(f'buy2buy: {len(top_15_buy2buy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df, top_20_clicks, top_clicks):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.1, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        \n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "    else:\n",
    "        products_1 = list(itertools.chain(*[top_20_clicks[product] \\\n",
    "                        for product in unique_products if product in top_20_clicks]))\n",
    "        top_products_1 = [product for product, _ in Counter(products_1).most_common(50) \\\n",
    "                        if product not in unique_products]\n",
    "        result = unique_products + top_products_1[:20 - len(unique_products)]\n",
    "        return result + list(top_clicks[:20 - len(result)])\n",
    "\n",
    "def suggest_buys(df, top_15_buy2buy, top_15_buys, top_orders):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # filter df for type 1 and 2\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        products_1 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                        for product in unique_buys if product in top_15_buy2buy]))\n",
    "        for product in products_1: products_tmp[product] += 0.1\n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "    else:\n",
    "        products_1 = list(itertools.chain(*[top_15_buys.get(product, []) \\\n",
    "                          for product in unique_products if product in top_15_buys]))\n",
    "        products_2 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                          for product in unique_buys if product in top_15_buy2buy]))\n",
    "        top_products = [product for product, _ in Counter(products_1 + products_2).most_common(50) \\\n",
    "                        if product not in unique_products]\n",
    "        result = unique_products + top_products[:20 - len(unique_products)]\n",
    "        return result + list(top_orders[:20 - len(result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 58s, sys: 0 ns, total: 11min 58s\n",
      "Wall time: 11min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_clicks = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x, top_20_clicks, top_clicks)\n",
    ")\n",
    "\n",
    "pred_df_buys = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x, top_15_buy2buy, top_15_buys, top_orders)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 884502 1157882 876129 118...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1715900 1647167 1339838 18819 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 583026 963957 254154 877...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 1159379 1202618 77906 75819...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  11098528_clicks  11830 588923 1732105 884502 1157882 876129 118...\n",
       "1  11098529_clicks  1105029 459126 1715900 1647167 1339838 18819 1...\n",
       "2  11098530_clicks  409236 264500 1603001 583026 963957 254154 877...\n",
       "3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n",
       "4  11098532_clicks  876469 7651 108125 1159379 1202618 77906 75819..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(config.data_path + 'submission.csv', index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.4746288024042827\n",
      "carts recall = 0.37670560399110464\n",
      "orders recall = 0.6320111840614357\n",
      "=============\n",
      "Overall Recall = 0.5396812718746211\n",
      "=============\n",
      "CPU times: user 47.3 s, sys: 471 ms, total: 47.8 s\n",
      "Wall time: 47.6 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# COMPUTE METRIC\n",
    "\n",
    "if config.local_validation:\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet(config.validation_path + 'test_labels.parquet')\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "        \n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clicks\n",
    "- 0.461\n",
    "- 0.474 (current parameters - clicks)\n",
    "### Carts\n",
    "- 0.376\n",
    "### Orders\n",
    "- 0.632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15824f91cc7b75e025d2367af0109417504b30ad993611e7f5b39069152fd433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
