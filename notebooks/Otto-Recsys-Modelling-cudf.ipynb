{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import cudf, itertools\n",
    "\n",
    "import pandarallel\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(nb_workers=8, progress_bar=True)\n",
    "\n",
    "# supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    data_path = '../data/'\n",
    "    local_validation = True\n",
    "    validation_path = '../data/local_validation/'\n",
    "    train_file = 'train.parquet'\n",
    "    test_file = 'test.parquet'\n",
    "    test_labels_file = 'test_labels.parquet'\n",
    "    n_session_samples = 100\n",
    "    n_most_common = 50\n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.local_validation:\n",
    "    train = cudf.read_parquet(config.validation_path + config.train_file)\n",
    "    test = cudf.read_parquet(config.validation_path + config.test_file)\n",
    "    test_labels = cudf.read_parquet(config.validation_path + config.test_labels_file)\n",
    "    data = cudf.concat([train, test])\n",
    "else:\n",
    "    train = cudf.read_parquet(config.data_path + config.train_file)\n",
    "    test = cudf.read_parquet(config.data_path + config.test_file)\n",
    "    data = cudf.concat([train, test])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debug:\n",
    "    data = data.sample(frac=0.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "type_weight = {0:1, 1:6, 2:3}\n",
    "version = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create co-visitation matrix on GPU using CuDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carts Orders Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 129/129 [00:52<00:00,  2.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code creates a dictionary mapping each product (aid) to the top 50 most similar products (aid_y) based on the type of product (type_y).\n",
    "# To do this, it first creates all possible pairs of products for each session. Then it calculates the weight of the pair based on type_y.\n",
    "# We then calculate the top 50 similar products for each product based on the sum of the weights of all the edges from the same source to target.\n",
    "\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "# as we are processing sessions in chunks\n",
    "# over iterations we can append duplicate aid_x and aid_y pairs\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'],ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n', axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = df.type_y.map(type_weight)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    # aid_x is the source, aid_y is the target\n",
    "    # calculate the sum of weights based on target type\n",
    "    # we sum the weights of all the edges from same source to target\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "tmp = tmp.groupby(['aid_x','aid_y']).wgt.sum().reset_index()\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "# we only select 50 products for each aid_x\n",
    "tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + f'top_15_carts_orders_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)\n",
    "    \n",
    "del df, tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buy2Buy Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 129/129 [00:34<00:00,  3.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df, on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    df['wgt'] = 1\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "       \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "tmp = tmp.groupby(['aid_x','aid_y']).wgt.sum().reset_index()\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "tmp.drop_duplicates(['aid_x','aid_y'],inplace=True)\n",
    "# SAVE TOP 15\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "# SAVE TO DISK\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + f'top_15_buy2buy_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)\n",
    "    \n",
    "del df, tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicks Co-visitation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 129/129 [00:55<00:00,  2.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%time\n",
    "data_copy = data.copy()\n",
    "data_copy = data_copy.set_index('session')\n",
    "sessions = data_copy.index.unique()\n",
    "\n",
    "chunk_size = 100_000\n",
    "\n",
    "tmp = list()\n",
    "for i in tqdm(range(0, sessions.shape[0], chunk_size)):\n",
    "    df = data_copy.loc[sessions[i]:sessions[min(sessions.shape[0]-1, i+chunk_size-1)]].reset_index()\n",
    "    df = df.sort_values(['session','ts'], ascending=[True, False])\n",
    "\n",
    "    # USE TAIL OF SESSION\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['n'] = df.groupby('session').cumcount()\n",
    "    df = df.loc[df.n<30].drop('n',axis=1)\n",
    "\n",
    "    # CREATE PAIRS\n",
    "    df = df.merge(df,on='session')\n",
    "    df = df.loc[ ((df.ts_x - df.ts_y).abs() < 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "\n",
    "    # ASSIGN WEIGHTS\n",
    "    df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "    # assign weights based on time\n",
    "    # 1 + 3*(ts_x - min(ts))/(max(ts)-min(ts))\n",
    "    # min(ts) = 1659304800\n",
    "    # max(ts) = 1662328791\n",
    "    # more recent the click, higher the weight\n",
    "    # factor can be a hyperparameter - to be finalized\n",
    "    df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "    df = df[['aid_x','aid_y','wgt']]\n",
    "    df.wgt = df.wgt.astype('float32')\n",
    "    df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "\n",
    "    tmp.append(df.reset_index())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "tmp = list(map(lambda x: x.to_pandas(), tmp))\n",
    "tmp = pd.concat(tmp)\n",
    "tmp.fillna(0,inplace=True)\n",
    "tmp = tmp.groupby(['aid_x','aid_y']).wgt.sum().reset_index()\n",
    "tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "tmp.drop_duplicates(['aid_x','aid_y'],inplace=True)\n",
    "tmp = tmp.reset_index(drop=True)\n",
    "tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n",
    "df = tmp.groupby('aid_x').aid_y.apply(list)\n",
    "with open(config.data_path + f'top_20_clicks_v{version}.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(), f)\n",
    "    \n",
    "del df, tmp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks: 1812132\n",
      "carts: 1812132\n",
      "buy2buy: 1055146\n",
      "CPU times: user 7.25 s, sys: 530 ms, total: 7.78 s\n",
      "Wall time: 7.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_20_clicks = pd.read_pickle(config.data_path + f'top_20_clicks_v{version}.pkl')\n",
    "top_15_buys = pd.read_pickle(config.data_path + f'top_15_carts_orders_v{version}.pkl')\n",
    "top_15_buy2buy = pd.read_pickle(config.data_path + f'top_15_buy2buy_v{version}.pkl')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test.loc[test['type']==type_labels['clicks'],'aid'].value_counts().index.values[:20]\n",
    "top_orders = test.loc[test['type']==type_labels['orders'],'aid'].value_counts().index.values[:20]\n",
    "\n",
    "# print shape of each matrix\n",
    "print(f'clicks: {len(top_20_clicks)}')\n",
    "print(f'carts: {len(top_15_buys)}')\n",
    "print(f'buy2buy: {len(top_15_buy2buy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_clicks(df, top_20_clicks, top_clicks):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.1, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        \n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "\n",
    "    # check if it is possible to index into dataframe using productid\n",
    "    products_1 = list(itertools.chain(*[top_20_clicks[product] \\\n",
    "                    for product in unique_products if product in top_20_clicks]))\n",
    "    top_products_1 = [product for product, _ in Counter(products_1).most_common(50) \\\n",
    "                    if product not in unique_products]\n",
    "    result = unique_products + top_products_1[:20 - len(unique_products)]\n",
    "    return result + list(top_clicks[:20 - len(result)])\n",
    "\n",
    "def suggest_buys(df, top_15_buy2buy, top_15_buys, top_orders):\n",
    "    products = df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # filter df for type 1 and 2\n",
    "    unique_products = list(dict.fromkeys(products[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys(df.aid.tolist()[::-1]))\n",
    "\n",
    "    if len(unique_products) >= 20:\n",
    "        weights = np.logspace(0.5, 1, len(products), base=2, endpoint=True) - 1\n",
    "        products_tmp = Counter()\n",
    "        for product, weight, _type in zip(products, weights, types):\n",
    "            products_tmp[product] += weight * type_weight[_type]\n",
    "        products_1 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                        for product in unique_buys if product in top_15_buy2buy]))\n",
    "        for product in products_1: products_tmp[product] += 0.1\n",
    "        sorted_products = [product for product, _ in products_tmp.most_common(50)]\n",
    "        return sorted_products\n",
    "\n",
    "    products_1 = list(itertools.chain(*[top_15_buys.get(product, []) \\\n",
    "                        for product in unique_products if product in top_15_buys]))\n",
    "    products_2 = list(itertools.chain(*[top_15_buy2buy.get(product, []) \\\n",
    "                        for product in unique_buys if product in top_15_buy2buy]))\n",
    "    top_products = [product for product, _ in Counter(products_1 + products_2).most_common(50) \\\n",
    "                    if product not in unique_products]\n",
    "    result = unique_products + top_products[:20 - len(unique_products)]\n",
    "    return result + list(top_orders[:20 - len(result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 1801251/1801251 [02:26<00:00, 12305.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 10.1 s, total: 2min 38s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# if not config.local_validation:\n",
    "test = test.to_pandas()\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "pred_df_clicks = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).progress_apply(\n",
    "    lambda x: suggest_clicks(x, top_20_clicks, top_clicks)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 1801251/1801251 [11:56<00:00, 2514.67it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df_buys = test.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).progress_apply(\n",
    "    lambda x: suggest_buys(x, top_15_buy2buy, top_15_buys, top_orders)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 5403753/5403753 [04:35<00:00, 19646.19it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "# pred_df = clicks_pred_df\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.progress_apply(lambda x: \" \".join(map(str,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.5255722760140219\n",
      "carts recall = 0.40959301417910704\n",
      "orders recall = 0.6489245235443005\n",
      "=============\n",
      "Overall Recall = 0.5647898459817146\n",
      "=============\n",
      "CPU times: user 42.6 s, sys: 982 ms, total: 43.6 s\n",
      "Wall time: 43.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# COMPUTE METRIC\n",
    "\n",
    "if config.local_validation:\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "    # for t in ['clicks']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:20]])\n",
    "        test_labels = pd.read_parquet('../data/local_validation/test_labels.parquet')\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "        \n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 1202618 1159379 77906 17040...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...\n",
       "1  11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...\n",
       "2  11098530_clicks  409236 264500 1603001 963957 254154 583026 167...\n",
       "3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n",
       "4  11098532_clicks  876469 7651 108125 1202618 1159379 77906 17040..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.to_csv(config.data_path + 'submission.csv', index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "15824f91cc7b75e025d2367af0109417504b30ad993611e7f5b39069152fd433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
